{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test sentence, that shouldn't be ignored or spamed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " 'sentence,',\n",
       " 'that',\n",
       " \"shouldn't\",\n",
       " 'be',\n",
       " 'ignored',\n",
       " 'or',\n",
       " 'spamed.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " 'sentence',\n",
       " 'that',\n",
       " 'shouldnt',\n",
       " 'be',\n",
       " 'ignored',\n",
       " 'or',\n",
       " 'spamed']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as reg\n",
    "# removing punctuation\n",
    "clean_text = reg.sub(r\"\\p{P}+\",\"\", text)\n",
    "clean_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test', 'sentence', ',', 'that', 'should', \"n't\", 'be', 'ignored', 'or', 'spamed', '.']\n"
     ]
    }
   ],
   "source": [
    "# loading english language model and applying spacy on it\n",
    "# this is an advanced tokenizer, that separates everything, even punctuations, shouldn't, :) or #python into tokens \n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', True), ('is', True), ('a', True), ('test', False), ('sentence', False), (',', False), ('that', True), ('should', True), (\"n't\", True), ('be', True), ('ignored', False), ('or', True), ('spamed', False), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "#finding stop words\n",
    "print([(token.text, token.is_stop) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', False), ('is', False), ('a', False), ('test', False), ('sentence', False), (',', True), ('that', False), ('should', False), (\"n't\", False), ('be', False), ('ignored', False), ('or', False), ('spamed', False), ('.', True)]\n"
     ]
    }
   ],
   "source": [
    "# finding punctuation\n",
    "print([(token.text, token.is_punct) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'this'), ('is', 'be'), ('a', 'a'), ('test', 'test'), ('sentence', 'sentence'), (',', ','), ('that', 'that'), ('should', 'should'), (\"n't\", 'not'), ('be', 'be'), ('ignored', 'ignore'), ('or', 'or'), ('spamed', 'spam'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Lemmantisation\n",
    "print([(token.text, token.lemma_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DET'), ('is', 'AUX'), ('a', 'DET'), ('test', 'NOUN'), ('sentence', 'NOUN'), (',', 'PUNCT'), ('that', 'DET'), ('should', 'VERB'), (\"n't\", 'PART'), ('be', 'AUX'), ('ignored', 'VERB'), ('or', 'CCONJ'), ('spamed', 'VERB'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "print([(token.text, token.pos_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
